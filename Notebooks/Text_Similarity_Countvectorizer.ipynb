{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = open(\"105-abraham-mi.txt\").read()\n",
    "df2 = open(\"105-akaka-hi.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocesser(text):\n",
    "    text= re.sub(r'\\W',' ', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text \n",
    "\n",
    "def stem_words(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocesser(df1)\n",
    "text_preprocesser(df2)\n",
    "df1 = stem_words(df1)\n",
    "df2 = stem_words(df2)\n",
    "df1 = lemmatize_words(df1)\n",
    "df2 = lemmatize_words(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(preprocessor = text_preprocesser, min_df =2)\n",
    "text_matrix = vectorizer.fit_transform([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YIGIT\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>60</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100th</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104th</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>york</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>young</th>\n",
       "      <td>62</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoo</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3235 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1\n",
       "000    289  36\n",
       "100     60  12\n",
       "100th    2   3\n",
       "104      2   1\n",
       "104th    8   5\n",
       "...    ...  ..\n",
       "york    17   3\n",
       "young   62  17\n",
       "youth   25  12\n",
       "zone    13   2\n",
       "zoo      2   1\n",
       "\n",
       "[3235 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.DataFrame(text_matrix.toarray().transpose(), index=vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity txt1 and txt2: [[0.82782669]]\n"
     ]
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#similarity_score = np.dot(text_matrix[0].toarray(), text_matrix[1].toarray().T)\n",
    "#print(\"Similarity score:\", similarity_score[0][0])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for i in range(1, 3):\n",
    "    globals()[\"txt\" + str(i)] =df[df.columns[i-1]].values.reshape(1, -1)\n",
    "print(\"Similarity txt1 and txt2:\", cosine_similarity(txt1, txt2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
